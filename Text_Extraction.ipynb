{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1VHTu228VkHrHAURVDjTmd5UOu763Fzgi",
      "authorship_tag": "ABX9TyO5fWmd9Wi/8cNOsDgXt6J7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshafi710/kttsdataenrichment/blob/main/Text_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UYmYSbCYxIBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a1dde3-e71e-40be-9085-9c49c2a1a308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.10/dist-packages (0.8)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: arabic_reshaper in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "# !pip install python-docx\n",
        "# import docx\n",
        "!pip install docx2txt\n",
        "import docx2txt\n",
        "from google.colab import drive\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import os\n",
        "import pathlib\n",
        "import logging\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "css_classes = \"\"\"\n",
        "<style>\n",
        "  h1, h2, h3, h4 {\n",
        "    color: red;\n",
        "    font-size: 24px;\n",
        "  }\n",
        "  \n",
        "  p {\n",
        "    font-size: 14px;\n",
        "    color: var(--text-color);\n",
        "  }\n",
        "</style>\n",
        "\"\"\"\n",
        "def  print_heading(text):\n",
        " display(HTML(f'''{css_classes}<h2> {text} </h2>'''))\n",
        "\n",
        "def print_text(text, color='grey'):\n",
        "  display (HTML(f''' {css_classes} <p style='--text-color:{color}'> {text}</p>'''))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K8ia0FBXQ3GW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORKING_DIR= '''/content/drive/MyDrive/PhD/Dataset/Text/text_files/'''\n",
        "VOCAB_FILENAME=\"/processed/kashmiri_vocab.csv\"\n",
        "ENRICHED_FILENAME_INITIAL=\"/processed/initial_kashmiri_enriched.csv\"\n",
        "ENRICHED_FILENAME_FINAL = \"/processed/final_kashmiri_enriched.csv\"\n",
        "LOG_FILE = '/processed/text-extraction.log'\n",
        "logging.basicConfig(filename=LOG_FILE, level=logging.INFO)\n",
        "#first letter in the TOKEN_DELIMITER_PATTERN is the Kashmiri fullstop character  \n",
        "TOKEN_DELIMITER_PATTERN = r'[۔;,:\\-\\s+\\n+\\(\\)\\\"\\?,]'\n",
        "SENTENCE_DELIMITER = r'[۔]'\n",
        "CORPUS_FILE = \"Edited Corpus.docx\"\n",
        "# CORPUS_FILE = 'Corpus316.docx'\n",
        "!ls $WORKING_DIR"
      ],
      "metadata": {
        "id": "0eF_6_XsxzSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cce6794-1cdd-4cac-e497-46f89b4cef2e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'corpus 1.gdoc'   Corpus297.docx   Corpus308.docx   Corpus319.docx\n",
            "'corpus 1.txt'\t  Corpus298.docx   Corpus309.docx   Corpus320.docx\n",
            " Corpus288.docx   Corpus299.docx   Corpus310.docx   Corpus321.docx\n",
            " Corpus289.docx   Corpus300.docx   Corpus311.docx   Corpus322.docx\n",
            " Corpus290.docx   Corpus301.docx   Corpus312.docx   Corpus323.docx\n",
            " Corpus291.docx   Corpus302.docx   Corpus313.docx   Corpus324.docx\n",
            " Corpus292.docx   Corpus303.docx   Corpus314.docx   Corpus325.docx\n",
            " Corpus293.docx   Corpus304.docx   Corpus315.docx  'Edited Corpus.docx'\n",
            " Corpus294.docx   Corpus305.docx   Corpus316.docx   processed\n",
            " Corpus295.docx   Corpus306.docx   Corpus317.docx   translated_files\n",
            " Corpus296.docx   Corpus307.docx   Corpus318.docx   translated_files.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab(corpus):\n",
        "  '''\n",
        "  return the vocabulary list (unique words in the given corpus)  '''  \n",
        "\n",
        "  trimmed_sentence = re.sub(TOKEN_DELIMITER_PATTERN, \" \", corpus)\n",
        "  tokens = trimmed_sentence.split() \n",
        "  print_heading(\"Overall Tokens \")\n",
        "  print_text (tokens)\n",
        "  print_heading(\"\\n------------------\")\n",
        "  print_text(\"\\nNo of Tokens (after excluding the punctuation): %d\"  % len(tokens))\n",
        "  tokens.sort()\n",
        "  # get the unique list using set operation\n",
        "  vocab = list(set(tokens))\n",
        "  vocab.sort()\n",
        "  print(\"Vocabulary Size: %d\" %len(vocab))\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "Jk2TJxrqCQwB"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_to_file(vocab):\n",
        "  ''' writes vocab to the file with name in the VOCAB_FILE_NAME in the working directory  set by the variable  WORKING_DIR'''\n",
        "  os.chdir(WORKING_DIR)\n",
        "  with open ( WORKING_DIR + VOCAB_FILENAME,'w') as out:\n",
        "      for token in vocab:\n",
        "        out.write(token+\"\\n\")\n",
        "  print_text(\"vocabulary stored in {} \\n \".format( WORKING_DIR + VOCAB_FILENAME))"
      ],
      "metadata": {
        "id": "B4tVW-51YgJ5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_richness(sentence):\n",
        "    '''return the basic richness of the sentence: basic richnes is \n",
        "    equal to number of unique words of a sentence'''  \n",
        "    #  replaces all occurrences of this pattern with a single space\n",
        "    trimmed_sentence = re.sub(TOKEN_DELIMITER_PATTERN, \" \", sentence)\n",
        "    tokens= trimmed_sentence.split()\n",
        "\n",
        "    print_text(\"\\n------In Richness ----{}----\\n\".format(len(tokens)),'cyan')\n",
        "    print (tokens)\n",
        "    print_text(\"\\n-----Out Rchness ------------\\n\",'cyan')\n",
        "    return  len(set(tokens))"
      ],
      "metadata": {
        "id": "Y9_A36i7CTqx"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO\n",
        "# If a word/senetence contains characters not in the list of kashmiri keyboard characters, reject that for simplicity purposes \n",
        "# Also delete the outliers from the dataset\n",
        "corpus =  docx2txt.process(WORKING_DIR +   CORPUS_FILE)\n",
        "vocab= get_vocab(corpus)\n",
        "vocab_to_file(vocab)"
      ],
      "metadata": {
        "id": "k5OBGrOi4CwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_richness_sentence_wise(corpus):\n",
        "    sentences = re.split(SENTENCE_DELIMITER,corpus)\n",
        "    print_heading('---------before processing-----------')\n",
        "    print(\" Total sentences: %d\" % len(sentences))\n",
        "    for  index, sentence  in enumerate(sentences):\n",
        "      print(\"{}th sntence : {}\".format(str(index), str(sentence)))\n",
        "\n",
        "    '''Filter out empty sentences while doing so, also replace the multiple \n",
        "     spaces with one space, remove parenthesis, commas , and quotes and alike\n",
        "      as mentioned in the token delimiter pattern'''\n",
        "    sentences = [re.sub(TOKEN_DELIMITER_PATTERN, \" \", sentence) for sentence in sentences if sentence.strip() ] \n",
        "    print_heading('-----------after processing---------')\n",
        "    for  index, sentence  in enumerate(sentences):\n",
        "      print(\"{}th sntence : {}\".format(str(index), str(sentence)))\n",
        "    print(\" Total sentences: %d\" % len(sentences))\n",
        "    os.chdir(WORKING_DIR)  \n",
        "    with open(WORKING_DIR + ENRICHED_FILENAME_INITIAL  ,'w') as out:\n",
        "      out.write('sentence,richness\\n')\n",
        "      for  sentence in sentences:\n",
        "        #exclude empty sentences\n",
        "        print('Length of a sentence is {}'.format(len(sentence)))\n",
        "        if len(sentence) >0:        \n",
        "          basic_richness= sentence_richness(sentence)\n",
        "          out.write(sentence +\",\" + str (basic_richness)+ \"\\n\")\n",
        "          print(sentence + \": \" +str(basic_richness) + \"\\n-------------------\\n\")"
      ],
      "metadata": {
        "id": "WMXgt7GJX3cY"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_richness_sentence_wise(corpus)"
      ],
      "metadata": {
        "id": "PGaAF9IN2cFl"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' reads the simple richness csv file pointed by RICHNESS_FILENAME into dataframe, sorts it  \n",
        "in descending order of richness, drops the rows with NA values in richness column in place\n",
        "''' \n",
        "df=pd.read_csv(WORKING_DIR+ENRICHED_FILENAME_INITIAL)\n",
        "df.columns=  ['sentence','richness']\n",
        "df=df.sort_values(by=['richness'], ascending=False)\n",
        "#drops the old index and make the changes in the df directly\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "#removes the dataframes where the richness column has missing or NaN values\n",
        "df=df.dropna(subset=['richness'])\n",
        "# add a new column to the data frame for setting the status of whether any of the tokens of the sentence has matched to the wishlist or not. \n",
        "# initially all sentences will have tokens which are in the wishlist. hence values are false.\n",
        "df=df.assign(Deleted=False)\n",
        "# file created for checking the temporary richness\n",
        "df.to_csv(WORKING_DIR + \"/processed/tmp_rich.csv\")"
      ],
      "metadata": {
        "id": "dM1CO3UHHYK7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ZDWaLho9gZoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' calculate the richness of the sentences itertively '''\n",
        "wishlist= set(vocab)\n",
        "print_heading(\"Vocab Length:{}\".format(len(wishlist)))\n",
        "for index, row in df.iterrows():\n",
        "    tokens= re.split(TOKEN_DELIMITER_PATTERN, row['sentence'])  \n",
        "    tokens = [token for token in tokens if token.strip() ]\n",
        "    tokens = set(tokens)\n",
        "    print_text(\"Total Unique Tokens: {}\".format(len(tokens)),'cyan')\n",
        "    print(tokens)\n",
        "\n",
        "    ''' richness of a given sentence is the number of unique tokens matching \n",
        "    with the remaining vocab list i.e., the intersection of the unique \n",
        "    tokens (set) in the sentence and the remaining vocab'''  \n",
        "\n",
        "    match_words = tokens & wishlist \n",
        "    not_matched = tokens - match_words\n",
        "    if len(match_words) > 0:\n",
        "      df.at[index,'richness'] = len(match_words)\n",
        "      print_text(\"\\n--------match words---{}----\\n\".format(len(match_words)),\"magenta\")     \n",
        "      print_text(match_words)\n",
        "\n",
        "      print_text(\"\\n--------Not matched words---{}----\\n\".format(len(not_matched)),\"wheat\")     \n",
        "      print_text(not_matched)\n",
        "      '''update the  wishlist by set difference operation of the wishlist \n",
        "      and the match  tokens: excluding the matching tokens from the wishlist'''\n",
        "      wishlist = wishlist - set(match_words)\n",
        "    else:\n",
        "      df.at[index,'Deleted']=True\n",
        "\n",
        "    print(\"Wishlist size remaining: %d \" %len(wishlist) )\n",
        "    print(\"DF size remaining: %d \" %len(df) )\n",
        "    print( \"\\n   ------------------- \")\n"
      ],
      "metadata": {
        "id": "BYH_0WE7HX2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_heading(\"sum of richness is {}\".format(df.loc[df['Deleted'] == False, 'richness'].sum()))\n",
        "print_heading (\"Lenth of vocab is {}\".format(len(vocab)))\n",
        "df"
      ],
      "metadata": {
        "id": "WhPzN9nBHXzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(WORKING_DIR + ENRICHED_FILENAME_FINAL)"
      ],
      "metadata": {
        "id": "9-J4dKOlDzDB"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}